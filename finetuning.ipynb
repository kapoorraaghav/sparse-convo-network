{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eed94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch : 2.12.0.dev20260217+cu128\n",
      "CUDA    : True\n",
      "GPU     : NVIDIA GeForce RTX 5060 Laptop GPU\n",
      "jet (60000, 125, 125, 8)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from model import *\n",
    "\n",
    "print(f\"PyTorch : {torch.__version__}\")\n",
    "print(f\"CUDA    : {torch.cuda.is_available()}\")\n",
    "print(f\"GPU     : {torch.cuda.get_device_name(0)}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "with h5py.File(\"dataset/Dataset_Specific_Unlabelled.h5\", \"r\") as f:\n",
    "    for key in f.keys():\n",
    "        print(key, f[key].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f52905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y (10000, 1) float32\n",
      "jet (10000, 125, 125, 8) float32\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(\"dataset/Dataset_Specific_labelled.h5\", \"r\") as f:\n",
    "    for key in f.keys():\n",
    "        print(key, f[key].shape, f[key].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a1aa98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (10000, 125, 125, 8)\n",
      "Y shape: (10000, 1)\n",
      "Y dtype: float32\n",
      "Y unique values: {0.0, 1.0}\n",
      "Y min: 0.0\n",
      "Y max: 1.0\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(\"dataset/Dataset_Specific_Labelled.h5\", \"r\") as f:\n",
    "    print(\"X shape:\", f[\"jet\"].shape)\n",
    "    print(\"Y shape:\", f[\"Y\"].shape)\n",
    "    print(\"Y dtype:\", f[\"Y\"].dtype)\n",
    "    y_sample = f[\"Y\"][:100].flatten().tolist() \n",
    "    print(\"Y unique values:\", set(y_sample))\n",
    "    print(\"Y min:\", min(y_sample))\n",
    "    print(\"Y max:\", max(y_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f4478e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ─── Classifier Model ─────────────────────────────────────────────────────────\n",
    "class SparseClassifier(nn.Module):\n",
    "    def __init__(self, in_channels=8):\n",
    "        super().__init__()\n",
    "\n",
    "        #Encoder\n",
    "        self.enc1  = SubmanifoldSparseConv2d(in_channels, 32,  3)\n",
    "        self.enc2  = SubmanifoldSparseConv2d(32,          64,  3)\n",
    "        self.down1 = StridedSparseConv2d    (64,          64,  3, stride=2)\n",
    "        self.enc3  = SubmanifoldSparseConv2d(64,          128, 3)\n",
    "        self.down2 = StridedSparseConv2d    (128,         128, 3, stride=2)\n",
    "\n",
    "        #Binary Classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),  \n",
    "            nn.Flatten(),        \n",
    "            nn.Dropout(0.9),    \n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.9),\n",
    "            nn.Linear(64, 32),    \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.9),\n",
    "            nn.Linear(32, 16),    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.9),\n",
    "            nn.Linear(8, 1)          \n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Encoder (frozen or fine-tuned)\n",
    "        z, m = self.enc1(x, mask);  z = self.relu(z)\n",
    "        z, m = self.enc2(z, m);     z = self.relu(z)\n",
    "        z, m = self.down1(z, m);    z = self.relu(z)\n",
    "        z, m = self.enc3(z, m);     z = self.relu(z)\n",
    "        z, m = self.down2(z, m);    z = self.relu(z)\n",
    "\n",
    "        z = z * m.float()\n",
    "\n",
    "        logit = self.classifier(z) \n",
    "        return logit.squeeze(1)     \n",
    "\n",
    "\n",
    "def load_pretrained_encoder(classifier, autoencoder_path):\n",
    "\n",
    "    checkpoint = torch.load(autoencoder_path, map_location=device)\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['model_state_dict']\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "    # Only load encoder layers\n",
    "    encoder_keys = ['enc1', 'enc2', 'down1', 'enc3', 'down2']\n",
    "    model_dict   = classifier.state_dict()\n",
    "\n",
    "    pretrained = {k: v for k, v in state_dict.items()\n",
    "                  if any(k.startswith(key) for key in encoder_keys)}\n",
    "\n",
    "    model_dict.update(pretrained)\n",
    "    classifier.load_state_dict(model_dict)\n",
    "\n",
    "    loaded = list(pretrained.keys())\n",
    "    print(f\"Loaded {len(loaded)} pretrained encoder weights\")\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f48701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class H5LabelledDataset(Dataset):\n",
    "    def __init__(self, file_path, x_key=\"jet\", y_key=\"Y\", threshold=0.0):\n",
    "        self.file_path = file_path\n",
    "        self.x_key     = x_key\n",
    "        self.y_key     = y_key\n",
    "        self.threshold = threshold\n",
    "        with h5py.File(file_path, \"r\") as f:\n",
    "            self.length = len(f[x_key])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.file_path, \"r\") as f:\n",
    "            x = torch.tensor(f[self.x_key][idx], dtype=torch.float32)\n",
    "            y = torch.tensor(f[self.y_key][idx], dtype=torch.float32).squeeze()  # (1,) → scalar\n",
    "        x    = x.permute(2, 0, 1)\n",
    "        mask = (x.abs().sum(dim=0, keepdim=True) > self.threshold)\n",
    "        return x, mask, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03560f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 8000 | Val: 2000\n",
      "Loaded 30 pretrained encoder weights\n",
      "Epochs 1-10: Encoder frozen, training head only\n",
      "Epoch 01 | Train Loss 0.7033 | Train Acc 50.31% | Val Loss 0.6934 | Val Acc 50.75%\n",
      "Epoch 02 | Train Loss 0.6927 | Train Acc 51.08% | Val Loss 0.6932 | Val Acc 50.75%\n",
      "Epoch 03 | Train Loss 0.6937 | Train Acc 51.19% | Val Loss 0.6930 | Val Acc 50.75%\n",
      "Epoch 04 | Train Loss 0.6934 | Train Acc 51.10% | Val Loss 0.6930 | Val Acc 50.75%\n",
      "Epoch 05 | Train Loss 0.6934 | Train Acc 51.02% | Val Loss 0.6931 | Val Acc 50.75%\n",
      "Epoch 06 | Train Loss 0.6931 | Train Acc 51.24% | Val Loss 0.6931 | Val Acc 50.75%\n",
      "Epoch 07 | Train Loss 0.6932 | Train Acc 51.18% | Val Loss 0.6931 | Val Acc 50.75%\n",
      "Epoch 08 | Train Loss 0.6927 | Train Acc 51.60% | Val Loss 0.6930 | Val Acc 50.75%\n",
      "Epoch 09 | Train Loss 0.6930 | Train Acc 51.10% | Val Loss 0.6931 | Val Acc 50.75%\n",
      "Epoch 10 | Train Loss 0.6927 | Train Acc 51.52% | Val Loss 0.6930 | Val Acc 50.75%\n",
      "\n",
      "Epoch 11: Encoder unfrozen, full fine-tuning\n",
      "Epoch 11 | Train Loss 0.6930 | Train Acc 50.95% | Val Loss 0.6931 | Val Acc 50.75%\n",
      "Epoch 12 | Train Loss 0.6924 | Train Acc 51.35% | Val Loss 0.6931 | Val Acc 50.75%\n",
      "Epoch 13 | Train Loss 0.6921 | Train Acc 52.29% | Val Loss 0.6931 | Val Acc 50.75%\n",
      "Epoch 14 | Train Loss 0.6925 | Train Acc 51.68% | Val Loss 0.6931 | Val Acc 50.75%\n",
      "Epoch 15 | Train Loss 0.6920 | Train Acc 52.71% | Val Loss 0.6931 | Val Acc 50.75%\n",
      "Epoch 16 | Train Loss 0.6913 | Train Acc 52.74% | Val Loss 0.6923 | Val Acc 50.75%\n",
      "Epoch 17 | Train Loss 0.6910 | Train Acc 53.17% | Val Loss 0.6929 | Val Acc 50.75%\n",
      "Epoch 18 | Train Loss 0.6905 | Train Acc 53.14% | Val Loss 0.6918 | Val Acc 50.75%\n",
      "Epoch 19 | Train Loss 0.6895 | Train Acc 53.80% | Val Loss 0.6894 | Val Acc 50.85%\n",
      "Epoch 20 | Train Loss 0.6883 | Train Acc 54.26% | Val Loss 0.6866 | Val Acc 53.70%\n",
      "Epoch 21 | Train Loss 0.6879 | Train Acc 54.34% | Val Loss 0.6885 | Val Acc 51.35%\n",
      "Epoch 22 | Train Loss 0.6855 | Train Acc 54.96% | Val Loss 0.6882 | Val Acc 51.45%\n",
      "Epoch 23 | Train Loss 0.6837 | Train Acc 55.53% | Val Loss 0.6869 | Val Acc 53.55%\n",
      "Epoch 24 | Train Loss 0.6803 | Train Acc 55.93% | Val Loss 0.6808 | Val Acc 59.75%\n",
      "Epoch 25 | Train Loss 0.6790 | Train Acc 56.45% | Val Loss 0.6690 | Val Acc 71.80%\n",
      "Epoch 26 | Train Loss 0.6783 | Train Acc 57.70% | Val Loss 0.6660 | Val Acc 74.20%\n",
      "Epoch 27 | Train Loss 0.6797 | Train Acc 57.31% | Val Loss 0.6589 | Val Acc 75.65%\n",
      "Epoch 28 | Train Loss 0.6687 | Train Acc 59.08% | Val Loss 0.6618 | Val Acc 74.80%\n",
      "Epoch 29 | Train Loss 0.6716 | Train Acc 58.15% | Val Loss 0.6579 | Val Acc 74.85%\n",
      "Epoch 30 | Train Loss 0.6747 | Train Acc 57.88% | Val Loss 0.6751 | Val Acc 61.35%\n",
      "Epoch 31 | Train Loss 0.6690 | Train Acc 59.41% | Val Loss 0.6366 | Val Acc 80.10%\n",
      "Epoch 32 | Train Loss 0.6677 | Train Acc 59.36% | Val Loss 0.6369 | Val Acc 80.60%\n",
      "Epoch 33 | Train Loss 0.6656 | Train Acc 59.51% | Val Loss 0.6374 | Val Acc 79.40%\n",
      "Epoch 34 | Train Loss 0.6628 | Train Acc 60.11% | Val Loss 0.6433 | Val Acc 77.40%\n",
      "Epoch 35 | Train Loss 0.6613 | Train Acc 60.31% | Val Loss 0.6158 | Val Acc 82.50%\n",
      "Epoch 36 | Train Loss 0.6666 | Train Acc 59.34% | Val Loss 0.6021 | Val Acc 78.35%\n",
      "Epoch 37 | Train Loss 0.6593 | Train Acc 60.20% | Val Loss 0.6277 | Val Acc 79.05%\n",
      "Epoch 38 | Train Loss 0.6573 | Train Acc 60.94% | Val Loss 0.6279 | Val Acc 80.20%\n",
      "Epoch 39 | Train Loss 0.6575 | Train Acc 61.31% | Val Loss 0.6398 | Val Acc 75.75%\n",
      "Epoch 40 | Train Loss 0.6540 | Train Acc 61.27% | Val Loss 0.6033 | Val Acc 78.90%\n",
      "Epoch 41 | Train Loss 0.6474 | Train Acc 61.94% | Val Loss 0.5863 | Val Acc 72.90%\n",
      "Epoch 42 | Train Loss 0.6516 | Train Acc 60.89% | Val Loss 0.6258 | Val Acc 78.85%\n",
      "Epoch 43 | Train Loss 0.6530 | Train Acc 61.14% | Val Loss 0.6000 | Val Acc 82.45%\n",
      "Epoch 44 | Train Loss 0.6439 | Train Acc 61.90% | Val Loss 0.6379 | Val Acc 75.50%\n",
      "Epoch 45 | Train Loss 0.6449 | Train Acc 62.54% | Val Loss 0.5920 | Val Acc 80.70%\n",
      "Epoch 46 | Train Loss 0.6464 | Train Acc 62.99% | Val Loss 0.5910 | Val Acc 80.30%\n",
      "Epoch 47 | Train Loss 0.6375 | Train Acc 62.81% | Val Loss 0.5947 | Val Acc 80.85%\n",
      "Epoch 48 | Train Loss 0.6366 | Train Acc 62.82% | Val Loss 0.5947 | Val Acc 82.55%\n",
      "Epoch 49 | Train Loss 0.6429 | Train Acc 62.76% | Val Loss 0.5971 | Val Acc 67.45%\n",
      "Epoch 50 | Train Loss 0.6349 | Train Acc 62.99% | Val Loss 0.6011 | Val Acc 78.75%\n",
      "\n",
      "Classifier saved → sparse_classifier.pth\n"
     ]
    }
   ],
   "source": [
    "#Fine-tuning Loop\n",
    "\n",
    "def finetune(autoencoder_path, labelled_file, \n",
    "             n_epochs=50, freeze_epochs=10):\n",
    "\n",
    "\n",
    "    dataset = H5LabelledDataset(labelled_file)\n",
    "\n",
    "    val_size   = int(0.2 * len(dataset))\n",
    "    train_size = len(dataset) - val_size\n",
    "    train_ds, val_ds = torch.utils.data.random_split(dataset, [train_size, val_size],generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=100, shuffle=True,  num_workers=0)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=100, shuffle=True, num_workers=0)\n",
    "\n",
    "    print(f\"Train: {train_size} | Val: {val_size}\")\n",
    "\n",
    "    # Model \n",
    "    classifier = SparseClassifier(in_channels=8).to(device)\n",
    "    classifier = load_pretrained_encoder(classifier, autoencoder_path)\n",
    "\n",
    "    #Loss\n",
    "    criterion = nn.BCEWithLogitsLoss() \n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        if epoch < freeze_epochs:\n",
    "            for name, param in classifier.named_parameters():\n",
    "                if any(name.startswith(k) for k in ['enc1','enc2','down1','enc3','down2']):\n",
    "                    param.requires_grad = False\n",
    "            optimizer = optim.SGD(\n",
    "                filter(lambda p: p.requires_grad, classifier.parameters()),\n",
    "                lr=1e-2, momentum=0.9, weight_decay=1e-4\n",
    "            )\n",
    "            if epoch == 0:\n",
    "                print(f\"Epochs 1-{freeze_epochs}: Encoder frozen, training head only\")\n",
    "\n",
    "        elif epoch == freeze_epochs:\n",
    "\n",
    "            for param in classifier.parameters():\n",
    "                param.requires_grad = True\n",
    "            optimizer = optim.Adam ([\n",
    "                {'params': classifier.enc1.parameters(),       'lr': 5e-4},\n",
    "                {'params': classifier.enc2.parameters(),       'lr': 5e-4},\n",
    "                {'params': classifier.down1.parameters(),      'lr': 5e-4},\n",
    "                {'params': classifier.enc3.parameters(),       'lr': 5e-4},\n",
    "                {'params': classifier.down2.parameters(),      'lr': 5e-4},\n",
    "                {'params': classifier.classifier.parameters(), 'lr': 5e-4   },\n",
    "            ], weight_decay=5e-4)\n",
    "            scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "            print(f\"\\nEpoch {epoch+1}: Encoder unfrozen, full fine-tuning\")\n",
    "\n",
    "        # Train\n",
    "        classifier.train()\n",
    "        train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "\n",
    "        for X, mask, y in train_loader:\n",
    "            X, mask, y = X.to(device), mask.to(device), y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = classifier(X, mask)      \n",
    "            loss   = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            preds         = (logits > 0).float() \n",
    "            train_correct += (preds == y).sum().item()\n",
    "            train_total   += y.size(0)\n",
    "            train_loss    += loss.item()\n",
    "\n",
    "        # Validate \n",
    "        classifier.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, mask, y in val_loader:\n",
    "                X, mask, y = X.to(device), mask.to(device), y.to(device)\n",
    "                logits      = classifier(X, mask)\n",
    "                loss        = criterion(logits, y)\n",
    "                preds       = (logits > 0).float()\n",
    "                val_correct += (preds == y).sum().item()\n",
    "                val_total   += y.size(0)\n",
    "                val_loss    += loss.item()\n",
    "\n",
    "        if epoch >= freeze_epochs:\n",
    "            scheduler.step()\n",
    "\n",
    "        train_acc = train_correct / train_total * 100\n",
    "        val_acc   = val_correct   / val_total   * 100\n",
    "\n",
    "        print(f\"Epoch {epoch+1:02d} | \"\n",
    "              f\"Train Loss {train_loss/len(train_loader):.4f} | \"\n",
    "              f\"Train Acc {train_acc:.2f}% | \"\n",
    "              f\"Val Loss {val_loss/len(val_loader):.4f} | \"\n",
    "              f\"Val Acc {val_acc:.2f}%\")\n",
    "\n",
    "    # Save\n",
    "    torch.save({\n",
    "        'model_state_dict': classifier.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': n_epochs,\n",
    "    }, \"models/sparse_classifier.pth\")\n",
    "    print(\"\\nClassifier saved → sparse_classifier.pth\")\n",
    "\n",
    "    return classifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "classifier = finetune(\n",
    "    autoencoder_path = \"sparse_autoencoder_checkpoint_2.pth\",\n",
    "    labelled_file    = \"Dataset_Specific_Labelled.h5\",\n",
    "    n_epochs         = 50,\n",
    "    freeze_epochs    = 10   # freeze encoder for first 10 epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067c5702",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
